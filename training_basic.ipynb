{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531e149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è A pasta 'train' j√° cont√©m arquivos. A extra√ß√£o foi abortada para evitar sobrescrita.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import extrair_zip_train_dir as zipService\n",
    "\n",
    "\n",
    "class ImageStitchingDatasetFiles(Dataset):\n",
    "    def __init__(self, folder_path, use_gradiente=False):\n",
    "        self.folder = Path(folder_path)\n",
    "        self.use_gradiente = use_gradiente\n",
    "        # Lista todos arquivos .pt ordenados\n",
    "        self.files = sorted(self.folder.glob(\"*.pt\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.load(self.files[idx])\n",
    "\n",
    "        def to_float_tensor(t):\n",
    "            # uint8 [0..255] -> float32 [0..1]\n",
    "            return t.float() / 255.0\n",
    "\n",
    "        parte1 = to_float_tensor(sample[\"parte1\"])\n",
    "        parte2 = to_float_tensor(sample[\"parte2\"])\n",
    "        groundtruth = to_float_tensor(sample[\"groundtruth\"])\n",
    "\n",
    "        if self.use_gradiente:\n",
    "            gradiente = to_float_tensor(sample[\"gradiente\"])\n",
    "            return (parte1, parte2), groundtruth, gradiente\n",
    "        else:\n",
    "            return (parte1, parte2), groundtruth\n",
    "\n",
    "zipService.descompactar_zip_com_progresso(\"./train.zip\", \"./train\")\n",
    "dataset = ImageStitchingDatasetFiles(\"./train\", use_gradiente=False)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39e6bb",
   "metadata": {},
   "source": [
    "32 x 48\n",
    "| Bloco                    | Altura √ó Largura | Canais p/ encoder | Canais P√≥s-concatena√ß√£o |\n",
    "|--------------------------|------------------|--------------------|--------------------------|\n",
    "| Entrada                  | 32√ó48            | 3                  | ‚Äî                        |\n",
    "| `enc1`                   | 32√ó48            | 32                 | 64 (concat)              |\n",
    "| `pool1`                  | 16√ó24            | 32                 | 64 (concat)              |\n",
    "| `enc2`                   | 16√ó24            | 64                 | 128 (concat)             |\n",
    "| `pool2`                  | 8√ó12             | 64                 | 128 (concat)             |\n",
    "| Bottleneck (concat)      | 8√ó12             | ‚Äî                  | 256                      |\n",
    "| `dec2` entrada           | 8√ó12             | 256 + 128 = 384    | ‚Äî                        |\n",
    "| `dec2` sa√≠da             | 16√ó24            | 64                 | ‚Äî                        |\n",
    "| `dec1` entrada           | 16√ó24            | 64 + 64 = 128      | ‚Äî                        |\n",
    "| `dec1` sa√≠da             | 32√ó48            | 32                 | ‚Äî                        |\n",
    "| Sa√≠da final              | 32√ó48            | 3                  | ‚Äî                        |\n",
    "\n",
    "64x96\n",
    "| Bloco                    | Altura √ó Largura | Canais p/ encoder | Canais P√≥s-concatena√ß√£o |\n",
    "|--------------------------|------------------|--------------------|--------------------------|\n",
    "| Entrada                  | 64√ó96            | 3                  | ‚Äî                        |\n",
    "| `enc1`                   | 64√ó96            | 32                 | 64 (concat)              |\n",
    "| `pool1`                  | 32√ó48            | 32                 | 64 (concat)              |\n",
    "| `enc2`                   | 32√ó48            | 64                 | 128 (concat)             |\n",
    "| `pool2`                  | 16√ó24            | 64                 | 128 (concat)             |\n",
    "| Bottleneck (concat)      | 16√ó24            | ‚Äî                  | 256                      |\n",
    "| `dec2` entrada           | 16√ó24            | 256 + 128 = 384    | ‚Äî                        |\n",
    "| `dec2` sa√≠da             | 32√ó48            | 64                 | ‚Äî                        |\n",
    "| `dec1` entrada           | 32√ó48            | 64 + 64 = 128      | ‚Äî                        |\n",
    "| `dec1` sa√≠da             | 64√ó96            | 32                 | ‚Äî                        |\n",
    "| Sa√≠da final              | 64√ó96            | 3                  | ‚Äî                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119f9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CBAM (Convolutional Block Attention Module)\n",
    "# Aplica aten√ß√£o canal + espacial separadamente\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.sigmoid_channel = nn.Sigmoid()\n",
    "        self.conv_spatial = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aten√ß√£o no canal\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        x_out = x * self.sigmoid_channel(avg_out + max_out)  # salva num novo tensor para n√£o perder o input original\n",
    "\n",
    "        # Aten√ß√£o espacial\n",
    "        avg_out = torch.mean(x_out, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x_out, dim=1, keepdim=True)\n",
    "        spatial_attention = torch.cat([avg_out, max_out], dim=1)  # [N, 2, H, W]\n",
    "        spatial_attention = self.sigmoid_spatial(self.conv_spatial(spatial_attention))  # [N, 1, H, W]\n",
    "\n",
    "        # Multiplica o resultado da aten√ß√£o espacial pelo tensor original (com canais corretos)\n",
    "        out = x_out * spatial_attention\n",
    "\n",
    "        return out\n",
    "\n",
    "# Self-Attention simples no bottleneck\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        proj_query = self.query(x).view(B, -1, H * W).permute(0, 2, 1)\n",
    "        proj_key = self.key(x).view(B, -1, H * W)\n",
    "        energy = torch.bmm(proj_query, proj_key)  # matriz de aten√ß√£o\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "\n",
    "        proj_value = self.value(x).view(B, -1, H * W)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(B, C, H, W)\n",
    "        return self.gamma * out + x\n",
    "\n",
    "# Bloco de codifica√ß√£o padr√£o\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Bloco de decodifica√ß√£o com upsample + concat + convolu√ß√µes\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_skip, ch_out):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(ch_in, ch_out, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_out + ch_skip, ch_out, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# Rede UNet com dois encoders, CBAM e self-attention no bottleneck\n",
    "class DualEncoderUNet_CBAM_SA_Small(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_ch=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Dois encoders independentes (parte1 e parte2)\n",
    "        self.enc1_1 = EncoderBlock(in_channels, base_ch)\n",
    "        self.enc2_1 = EncoderBlock(base_ch, base_ch * 2)\n",
    "\n",
    "        self.enc1_2 = EncoderBlock(in_channels, base_ch)\n",
    "        self.enc2_2 = EncoderBlock(base_ch, base_ch * 2)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck com self-attention\n",
    "        self.bottleneck = EncoderBlock(base_ch * 4, base_ch * 4)\n",
    "        self.attn = SelfAttention(base_ch * 4)\n",
    "\n",
    "        # CBAM nas skip connections\n",
    "        self.cbam2 = CBAM(base_ch * 4)\n",
    "        self.cbam1 = CBAM(base_ch * 2)\n",
    "\n",
    "        # Decoder com tr√™s par√¢metros por bloco\n",
    "        self.dec2 = DecoderBlock(base_ch * 4, base_ch * 4, base_ch * 2)  # 128, 128, 64\n",
    "        self.dec1 = DecoderBlock(base_ch * 2, base_ch * 2, base_ch)      # 64, 64, 32\n",
    "\n",
    "        self.final = nn.Conv2d(base_ch, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Encoder para parte1\n",
    "        e1_1 = self.enc1_1(x1)\n",
    "        e2_1 = self.enc2_1(self.pool(e1_1))\n",
    "\n",
    "        # Encoder para parte2\n",
    "        e1_2 = self.enc1_2(x2)\n",
    "        e2_2 = self.enc2_2(self.pool(e1_2))\n",
    "\n",
    "        # Garantir que as features estejam com mesmas dimens√µes (por seguran√ßa)\n",
    "        if e1_1.shape[2:] != e1_2.shape[2:]:\n",
    "            e1_2 = F.interpolate(e1_2, size=e1_1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        if e2_1.shape[2:] != e2_2.shape[2:]:\n",
    "            e2_2 = F.interpolate(e2_2, size=e2_1.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Bottleneck: concatena√ß√£o + aten√ß√£o\n",
    "        b = self.bottleneck(torch.cat([self.pool(e2_1), self.pool(e2_2)], dim=1))\n",
    "        b = self.attn(b)\n",
    "\n",
    "        if debug > 0: print(\"b shape:\", b.shape)\n",
    "        if debug > 0: print(\"skip2 shape:\", self.cbam2(torch.cat([e2_1, e2_2], dim=1)).shape)\n",
    "        if debug > 0: print(\"skip1 shape:\", self.cbam1(torch.cat([e1_1, e1_2], dim=1)).shape)\n",
    "\n",
    "\n",
    "        # Decoder com CBAM nas skip connections\n",
    "        d2 = self.dec2(b, self.cbam2(torch.cat([e2_1, e2_2], dim=1)))\n",
    "        d1 = self.dec1(d2, self.cbam1(torch.cat([e1_1, e1_2], dim=1)))\n",
    "\n",
    "        return torch.sigmoid(self.final(d1))  # sa√≠da com valo\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=9):  # Agora espera parte1 (3) + parte2 (3) + target/fake (3)\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(in_channels, 64, normalize=False),  # in_channels = 9\n",
    "            *block(64, 128),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)  # sa√≠da do PatchGAN (mapa de decis√£o)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d415f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_msssim import ssim, ms_ssim\n",
    "import lpips\n",
    "from metrics import compute_all_metrics  # Importa a fun√ß√£o de m√©tricas do arquivo metrics.py\n",
    "from tqdm import tqdm\n",
    "\n",
    "debug = 0\n",
    "# from generator import DualEncoderUNet_CBAM_SA_Small  # novo gerador com 2 encoders, CBAM e SelfAttention\n",
    "# from discriminator import PatchDiscriminator  # ou caminho equivalente\n",
    "\n",
    "# LPIPS usa um modelo de rede para compara√ß√£o perceptual\n",
    "lpips_fn = lpips.LPIPS(net='alex')\n",
    "\n",
    "def carregar_checkpoint_mais_recente(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_epoch\") and f.endswith(\".pt\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"epoch\")[1].split(\".\")[0]))\n",
    "    return os.path.join(checkpoint_dir, checkpoints[-1])\n",
    "\n",
    "\n",
    "def train(\n",
    "    generator, discriminator, dataloader, device, epochs,\n",
    "    save_every, checkpoint_dir, checkpoint_batch_dir,\n",
    "    tensorboard_dir, metrics, lr_g=2e-4, lr_d=2e-4,\n",
    "    lr_min=1e-6, gen_steps_per_batch=1\n",
    "):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_batch_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(tensorboard_dir)\n",
    "\n",
    "    criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "\n",
    "    scheduler_G = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=epochs, eta_min=lr_min)\n",
    "    scheduler_D = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_D, T_max=epochs, eta_min=lr_min)\n",
    "\n",
    "    start_epoch = 0\n",
    "    checkpoint_path = carregar_checkpoint_mais_recente(checkpoint_dir)\n",
    "    if checkpoint_path:\n",
    "        print(f\"üîÅ Carregando checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
    "        discriminator.load_state_dict(checkpoint[\"discriminator_state_dict\"])\n",
    "        optimizer_G.load_state_dict(checkpoint[\"optimizer_G_state_dict\"])\n",
    "        optimizer_D.load_state_dict(checkpoint[\"optimizer_D_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"‚úîÔ∏è Retomando a partir da √©poca {start_epoch}\")\n",
    "\n",
    "    last_checkpoint_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for i, ((part1, part2), target) in pbar:            \n",
    "            part1 = part1.to(device)\n",
    "            part2 = part2.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            real_input = torch.cat([part1, part2, target], dim=1)\n",
    "            fake = generator(part1, part2)\n",
    "            fake_input = torch.cat([part1, part2, fake.detach()], dim=1)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            pred_real = discriminator(real_input)\n",
    "            pred_fake = discriminator(fake_input)\n",
    "\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "            loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            for _ in range(gen_steps_per_batch):\n",
    "                fake = generator(part1, part2)\n",
    "                fake_input = torch.cat([part1, part2, fake], dim=1)\n",
    "                optimizer_G.zero_grad()\n",
    "                pred_fake = discriminator(fake_input)\n",
    "                loss_G_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "                loss_G_L1 = criterion_L1(fake, target)\n",
    "                loss_G = 8.0 * loss_G_GAN + 2.0 * loss_G_L1\n",
    "                loss_G.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss_G\": f\"{loss_G.item():.4f}\",\n",
    "                \"loss_D\": f\"{loss_D.item():.4f}\"\n",
    "            })\n",
    "\n",
    "            writer.add_scalar(\"Loss/Generator\", loss_G.item(), epoch * len(dataloader) + i)\n",
    "            writer.add_scalar(\"Loss/Discriminator\", loss_D.item(), epoch * len(dataloader) + i)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                eval_metrics = compute_all_metrics(fake, target, part1, part2, writer, epoch * len(dataloader) + i)\n",
    "                for k, v in eval_metrics.items():\n",
    "                    if v is not None:\n",
    "                        writer.add_scalar(f\"Metrics/{k}\", v, epoch * len(dataloader) + i)\n",
    "\n",
    "            # Checkpoint a cada 10 minutos\n",
    "            if time.time() - last_checkpoint_time > 600:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': i,\n",
    "                    'generator_state_dict': generator.state_dict(),\n",
    "                    'discriminator_state_dict': discriminator.state_dict(),\n",
    "                    'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                    'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                }, os.path.join(checkpoint_batch_dir, f'checkpoint_epoch{epoch}_batch{i}.pt'))\n",
    "                last_checkpoint_time = time.time()\n",
    "\n",
    "        # Fim da √©poca: salvar checkpoint principal\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        }, os.path.join(checkpoint_dir, f'checkpoint_epoch{epoch}.pt'))\n",
    "\n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0828f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/1 [00:00<?, ?it/s]/tmp/ipykernel_353957/2836379704.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sample = torch.load(self.files[idx])\n",
      "Epoch 1/200: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:05<00:00,  5.22s/it, loss_G=2.4001, loss_D=0.7041]\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7aaebeb40720>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/multiprocessing/connection.py\", line 133, in __del__\n",
      "    self._close()\n",
      "  File \"/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7aaebeb40720>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/multiprocessing/connection.py\", line 133, in __del__\n",
      "    self._close()\n",
      "  File \"/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m generator \u001b[38;5;241m=\u001b[39m DualEncoderUNet_CBAM_SA_Small()\n\u001b[1;32m     21\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m PatchDiscriminator()\n\u001b[0;32m---> 22\u001b[0m train(generator, discriminator, dataloader, device, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, save_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m     23\u001b[0m       checkpoint_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, checkpoint_batch_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m       tensorboard_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs/32x48\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, gen_steps_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 122\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(generator, discriminator, dataloader, device, epochs, save_every, checkpoint_dir, checkpoint_batch_dir, tensorboard_dir, metrics, lr_g, lr_d, lr_min, gen_steps_per_batch)\u001b[0m\n\u001b[1;32m    119\u001b[0m         last_checkpoint_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Fim da √©poca: salvar checkpoint principal\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: generator\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscriminator_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: discriminator\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_G_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer_G\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_D_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer_D\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    128\u001b[0m }, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    130\u001b[0m scheduler_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    131\u001b[0m scheduler_D\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/torch/serialization.py:652\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/torch/serialization.py:886\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    885\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 886\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(name, storage, num_bytes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "\n",
    "\n",
    "# Montar o caminho do checkpoint e logs para o tensorboard\n",
    "!sshfs prkdvps@64.71.153.122:/home/prkdvps/tensorboard/logs /home/prkd/gan-image-stitching-training/gan_image_stitching_training/logs\n",
    "!sshfs prkdvps@64.71.153.122:/home/prkdvps/tensorboard/checkpoints_epoch /home/prkd/gan-image-stitching-training/gan_image_stitching_training/checkpoints_epoch/\n",
    "!sshfs prkdvps@64.71.153.122:/home/prkdvps/tensorboard/checkpoints_batch/ /home/prkd/gan-image-stitching-training/gan_image_stitching_training/checkpoints_batch/\n",
    "\n",
    "\n",
    "# Hiperpar√¢metros\n",
    "num_epochs = 100\n",
    "gen_steps_per_batch = 20\n",
    "learning_rate = 2e-4\n",
    "lr_min = 1e-5\n",
    "lr_max = 2e-4\n",
    "log_interval = 600  # em segundos (10 minutos)\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Exemplo de chamada (fora do train.py):\n",
    "generator = DualEncoderUNet_CBAM_SA_Small()\n",
    "discriminator = PatchDiscriminator()\n",
    "train(generator, discriminator, dataloader, device, epochs=200, save_every=600,\n",
    "      checkpoint_dir=\"./checkpoints_epoch\", checkpoint_batch_dir=\"./checkpoints_batch\",\n",
    "      tensorboard_dir=\"./logs/32x48\", metrics=True, gen_steps_per_batch=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualizar_amostra_pt(caminho_pt):\n",
    "    \"\"\"\n",
    "    Visualiza a amostra salva no arquivo .pt no formato esperado:\n",
    "    dicion√°rio com chaves: 'parte1', 'parte2', 'mask', 'groundtruth', 'gradiente'.\n",
    "    Cada tensor √© uint8, shape [C, H, W].\n",
    "\n",
    "    Par√¢metros:\n",
    "        caminho_pt (str ou Path): caminho do arquivo .pt a ser aberto\n",
    "    \"\"\"\n",
    "    sample = torch.load(caminho_pt)\n",
    "\n",
    "    print(\"Chaves no arquivo:\", list(sample.keys()))\n",
    "    for k, v in sample.items():\n",
    "        print(f\"{k}: shape {v.shape}, dtype {v.dtype}\")\n",
    "\n",
    "    # Converter para formato H x W x C e mostrar com matplotlib\n",
    "    def tensor_to_img(tensor):\n",
    "        # tensor [C, H, W], uint8\n",
    "        img = tensor.permute(1, 2, 0).cpu().numpy()\n",
    "        return img\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    for i, key in enumerate(['parte1', 'parte2', 'mask', 'groundtruth', 'gradiente'], 1):\n",
    "        if key in sample:\n",
    "            img = tensor_to_img(sample[key])\n",
    "            shape_str = sample[key].shape\n",
    "            plt.subplot(2, 3, i)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"{key} - {shape_str}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualizar_amostra_pt(\"./train/000000009286_sample10.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageStitching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
