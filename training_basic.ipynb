{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import extrair_zip_train_dir as zipService\n",
    "\n",
    "\n",
    "class ImageStitchingDatasetFiles(Dataset):\n",
    "    def __init__(self, folder_path, use_gradiente=False):\n",
    "        self.folder = Path(folder_path)\n",
    "        self.use_gradiente = use_gradiente\n",
    "        # Lista todos arquivos .pt ordenados\n",
    "        self.files = sorted(self.folder.glob(\"*.pt\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.load(self.files[idx])\n",
    "\n",
    "        def to_float_tensor(t):\n",
    "            # uint8 [0..255] -> float32 [0..1]\n",
    "            return t.float() / 255.0\n",
    "\n",
    "        parte1 = to_float_tensor(sample[\"parte1\"])\n",
    "        parte2 = to_float_tensor(sample[\"parte2\"])\n",
    "        groundtruth = to_float_tensor(sample[\"groundtruth\"])\n",
    "\n",
    "        if self.use_gradiente:\n",
    "            gradiente = to_float_tensor(sample[\"gradiente\"])\n",
    "            return (parte1, parte2), groundtruth, gradiente\n",
    "        else:\n",
    "            return (parte1, parte2), groundtruth\n",
    "\n",
    "zipService.descompactar_zip_com_progresso(\"./train.zip\", \"./train\")\n",
    "dataset = ImageStitchingDatasetFiles(\"./train\", use_gradiente=False)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39e6bb",
   "metadata": {},
   "source": [
    "32 x 48\n",
    "| Bloco                    | Altura × Largura | Canais p/ encoder | Canais Pós-concatenação |\n",
    "|--------------------------|------------------|--------------------|--------------------------|\n",
    "| Entrada                  | 32×48            | 3                  | —                        |\n",
    "| `enc1`                   | 32×48            | 32                 | 64 (concat)              |\n",
    "| `pool1`                  | 16×24            | 32                 | 64 (concat)              |\n",
    "| `enc2`                   | 16×24            | 64                 | 128 (concat)             |\n",
    "| `pool2`                  | 8×12             | 64                 | 128 (concat)             |\n",
    "| Bottleneck (concat)      | 8×12             | —                  | 256                      |\n",
    "| `dec2` entrada           | 8×12             | 256 + 128 = 384    | —                        |\n",
    "| `dec2` saída             | 16×24            | 64                 | —                        |\n",
    "| `dec1` entrada           | 16×24            | 64 + 64 = 128      | —                        |\n",
    "| `dec1` saída             | 32×48            | 32                 | —                        |\n",
    "| Saída final              | 32×48            | 3                  | —                        |\n",
    "\n",
    "64x96\n",
    "| Bloco                    | Altura × Largura | Canais p/ encoder | Canais Pós-concatenação |\n",
    "|--------------------------|------------------|--------------------|--------------------------|\n",
    "| Entrada                  | 64×96            | 3                  | —                        |\n",
    "| `enc1`                   | 64×96            | 32                 | 64 (concat)              |\n",
    "| `pool1`                  | 32×48            | 32                 | 64 (concat)              |\n",
    "| `enc2`                   | 32×48            | 64                 | 128 (concat)             |\n",
    "| `pool2`                  | 16×24            | 64                 | 128 (concat)             |\n",
    "| Bottleneck (concat)      | 16×24            | —                  | 256                      |\n",
    "| `dec2` entrada           | 16×24            | 256 + 128 = 384    | —                        |\n",
    "| `dec2` saída             | 32×48            | 64                 | —                        |\n",
    "| `dec1` entrada           | 32×48            | 64 + 64 = 128      | —                        |\n",
    "| `dec1` saída             | 64×96            | 32                 | —                        |\n",
    "| Saída final              | 64×96            | 3                  | —                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CBAM (Convolutional Block Attention Module)\n",
    "# Aplica atenção canal + espacial separadamente\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.sigmoid_channel = nn.Sigmoid()\n",
    "        self.conv_spatial = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Atenção no canal\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        x = x * self.sigmoid_channel(avg_out + max_out)\n",
    "\n",
    "        # Atenção espacial\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = x * self.sigmoid_spatial(self.conv_spatial(x))\n",
    "        return x\n",
    "\n",
    "# Self-Attention simples no bottleneck\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        proj_query = self.query(x).view(B, -1, H * W).permute(0, 2, 1)\n",
    "        proj_key = self.key(x).view(B, -1, H * W)\n",
    "        energy = torch.bmm(proj_query, proj_key)  # matriz de atenção\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "\n",
    "        proj_value = self.value(x).view(B, -1, H * W)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(B, C, H, W)\n",
    "        return self.gamma * out + x\n",
    "\n",
    "# Bloco de codificação padrão\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Bloco de decodificação com upsample + concat + convoluções\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)  # concatena skip connection\n",
    "        return self.conv(x)\n",
    "\n",
    "# Rede UNet com dois encoders, CBAM e self-attention no bottleneck\n",
    "class DualEncoderUNet_CBAM_SA_Small(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_ch=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Dois encoders independentes (parte1 e parte2)\n",
    "        self.enc1_1 = EncoderBlock(in_channels, base_ch)\n",
    "        self.enc2_1 = EncoderBlock(base_ch, base_ch * 2)\n",
    "\n",
    "        self.enc1_2 = EncoderBlock(in_channels, base_ch)\n",
    "        self.enc2_2 = EncoderBlock(base_ch, base_ch * 2)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck com self-attention\n",
    "        self.bottleneck = EncoderBlock(base_ch * 4, base_ch * 4)\n",
    "        self.attn = SelfAttention(base_ch * 4)\n",
    "\n",
    "        # CBAM nas skip connections\n",
    "        self.cbam2 = CBAM(base_ch * 4)  # corrigido para concatenação dos dois caminhos\n",
    "        self.cbam1 = CBAM(base_ch * 2)\n",
    "\n",
    "        # Decoder reduzido (dois níveis), corrigido para lidar com concatenações\n",
    "        self.dec2 = DecoderBlock(base_ch * 8, base_ch * 2)  # 4 (bottleneck) + 4 (skip)\n",
    "        self.dec1 = DecoderBlock(base_ch * 4, base_ch)      # 2 + 2\n",
    "\n",
    "        self.final = nn.Conv2d(base_ch, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Encoder parte 1\n",
    "        e1_1 = self.enc1_1(x1)\n",
    "        e2_1 = self.enc2_1(self.pool(e1_1))\n",
    "\n",
    "        # Encoder parte 2\n",
    "        e1_2 = self.enc1_2(x2)\n",
    "        e2_2 = self.enc2_2(self.pool(e1_2))\n",
    "\n",
    "        # Garantir que as features tenham mesmo tamanho\n",
    "        if e1_1.shape[2:] != e1_2.shape[2:]:\n",
    "            e1_2 = F.interpolate(e1_2, size=e1_1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        if e2_1.shape[2:] != e2_2.shape[2:]:\n",
    "            e2_2 = F.interpolate(e2_2, size=e2_1.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Bottleneck com concatenação e self-attention\n",
    "        b = self.bottleneck(torch.cat([self.pool(e2_1), self.pool(e2_2)], dim=1))\n",
    "        b = self.attn(b)\n",
    "\n",
    "        # Decoder com CBAM nas skip connections (corrigido para canais concatenados)\n",
    "        d2 = self.dec2(b, self.cbam2(torch.cat([e2_1, e2_2], dim=1)))\n",
    "        d1 = self.dec1(d2, self.cbam1(torch.cat([e1_1, e1_2], dim=1)))\n",
    "\n",
    "        return torch.sigmoid(self.final(d1))  # saída com sigmoid (valores entre 0 e 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d415f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToPILImage\n",
    "from datetime import datetime\n",
    "\n",
    "from generator import DualEncoderUNet_CBAM_SA_Small\n",
    "from discriminator import Discriminator\n",
    "from losses import GANLoss, compute_gradient_penalty\n",
    "from metrics import compute_all_metrics\n",
    "\n",
    "# === Hiperparâmetros ===\n",
    "min_lr = 1e-5\n",
    "max_lr = 2e-4\n",
    "gen_steps = 20\n",
    "\n",
    "\n",
    "def train(dataloader, device, num_epochs=100, log_interval=600):\n",
    "    # Diretórios\n",
    "    os.makedirs(\"checkpoints_epoch\", exist_ok=True)\n",
    "    os.makedirs(\"checkpoints_batch\", exist_ok=True)\n",
    "    logdir = os.path.join(\"runs\", datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    writer = SummaryWriter(logdir)\n",
    "\n",
    "    # Modelos\n",
    "    G = DualEncoderUNet_CBAM_SA_Small().to(device)\n",
    "    D = Discriminator().to(device)\n",
    "\n",
    "    # Otimizadores com LR inicial máximo\n",
    "    opt_G = torch.optim.Adam(G.parameters(), lr=max_lr, betas=(0.5, 0.999))\n",
    "    opt_D = torch.optim.Adam(D.parameters(), lr=max_lr, betas=(0.5, 0.999))\n",
    "\n",
    "    # Critério adversarial\n",
    "    gan_loss = GANLoss().to(device)\n",
    "\n",
    "    # Timer\n",
    "    last_log_time = time.time()\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in dataloader:\n",
    "            G.train(); D.train()\n",
    "            (p1, p2), gt = batch\n",
    "            p1, p2, gt = p1.to(device), p2.to(device), gt.to(device)\n",
    "\n",
    "            # === Treinar Discriminador ===\n",
    "            with torch.no_grad():\n",
    "                fake = G(p1, p2).detach()\n",
    "            real_input = torch.cat([p1, p2, gt], dim=1)\n",
    "            fake_input = torch.cat([p1, p2, fake], dim=1)\n",
    "\n",
    "            pred_real = D(real_input)\n",
    "            pred_fake = D(fake_input)\n",
    "\n",
    "            loss_D = gan_loss.discriminator_loss(pred_real, pred_fake)\n",
    "            gp = compute_gradient_penalty(D, real_input, fake_input, device)\n",
    "            loss_D_total = loss_D + 10 * gp\n",
    "\n",
    "            opt_D.zero_grad()\n",
    "            loss_D_total.backward()\n",
    "            opt_D.step()\n",
    "\n",
    "            # === Treinar Gerador ===\n",
    "            for _ in range(gen_steps):\n",
    "                fake = G(p1, p2)\n",
    "                fake_input = torch.cat([p1, p2, fake], dim=1)\n",
    "\n",
    "                pred_fake = D(fake_input)\n",
    "                loss_G_GAN = gan_loss.generator_loss(pred_fake)\n",
    "                loss_G_L1 = nn.L1Loss()(fake, gt)\n",
    "                loss_G = 8 * loss_G_GAN + 2 * loss_G_L1\n",
    "\n",
    "                opt_G.zero_grad()\n",
    "                loss_G.backward()\n",
    "                opt_G.step()\n",
    "\n",
    "            # === Ajuste de LR (cosine decay) ===\n",
    "            progress = epoch / num_epochs\n",
    "            lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + torch.cos(torch.tensor(progress * 3.1416)))\n",
    "            for param_group in opt_G.param_groups:\n",
    "                param_group['lr'] = lr.item()\n",
    "            for param_group in opt_D.param_groups:\n",
    "                param_group['lr'] = lr.item()\n",
    "\n",
    "            # === TensorBoard ===\n",
    "            writer.add_scalar(\"Loss/Discriminator\", loss_D.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/Generator\", loss_G.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/L1\", loss_G_L1.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/GAN\", loss_G_GAN.item(), global_step)\n",
    "            writer.add_scalar(\"GP\", gp.item(), global_step)\n",
    "            writer.add_scalar(\"LR\", lr.item(), global_step)\n",
    "\n",
    "            # === Métricas ===\n",
    "            metrics = compute_all_metrics(fake, gt)\n",
    "            for k, v in metrics.items():\n",
    "                writer.add_scalar(f\"Metrics/{k}\", v, global_step)\n",
    "\n",
    "            # === Visualização ===\n",
    "            if global_step % 50 == 0:\n",
    "                grid = make_grid(torch.cat([p1, p2, fake, gt], dim=0), nrow=p1.size(0))\n",
    "                writer.add_image(\"Comparison\", grid, global_step)\n",
    "\n",
    "            # === Checkpoint por tempo ===\n",
    "            if time.time() - last_log_time > log_interval:\n",
    "                torch.save(G.state_dict(), f\"checkpoints_batch/G_step{global_step}.pt\")\n",
    "                torch.save(D.state_dict(), f\"checkpoints_batch/D_step{global_step}.pt\")\n",
    "                last_log_time = time.time()\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        # === Checkpoint por época ===\n",
    "        torch.save(G.state_dict(), f\"checkpoints_epoch/G_epoch{epoch}.pt\")\n",
    "        torch.save(D.state_dict(), f\"checkpoints_epoch/D_epoch{epoch}.pt\")\n",
    "\n",
    "    writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
