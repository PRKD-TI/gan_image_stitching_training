{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531e149e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ A pasta 'train' já contém arquivos. A extração foi abortada para evitar sobrescrita.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import extrair_zip_train_dir as zipService\n",
    "\n",
    "\n",
    "class ImageStitchingDatasetFiles(Dataset):\n",
    "    def __init__(self, folder_path, use_gradiente=False):\n",
    "        self.folder = Path(folder_path)\n",
    "        self.use_gradiente = use_gradiente\n",
    "        # Lista todos arquivos .pt ordenados\n",
    "        self.files = sorted(self.folder.glob(\"*.pt\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.load(self.files[idx])\n",
    "\n",
    "        def to_float_tensor(t):\n",
    "            # uint8 [0..255] -> float32 [0..1]\n",
    "            return t.float() / 255.0\n",
    "\n",
    "        parte1 = to_float_tensor(sample[\"parte1\"])\n",
    "        parte2 = to_float_tensor(sample[\"parte2\"])\n",
    "        groundtruth = to_float_tensor(sample[\"groundtruth\"])\n",
    "\n",
    "        if self.use_gradiente:\n",
    "            gradiente = to_float_tensor(sample[\"gradiente\"])\n",
    "            return (parte1, parte2), groundtruth, gradiente\n",
    "        else:\n",
    "            return (parte1, parte2), groundtruth\n",
    "\n",
    "zipService.descompactar_zip_com_progresso(\"./train.zip\", \"./train\")\n",
    "dataset = ImageStitchingDatasetFiles(\"./train\", use_gradiente=False)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39e6bb",
   "metadata": {},
   "source": [
    "32 x 48\n",
    "| Bloco                    | Altura × Largura | Canais p/ encoder | Canais Pós-concatenação |\n",
    "|--------------------------|------------------|--------------------|--------------------------|\n",
    "| Entrada                  | 32×48            | 3                  | —                        |\n",
    "| `enc1`                   | 32×48            | 32                 | 64 (concat)              |\n",
    "| `pool1`                  | 16×24            | 32                 | 64 (concat)              |\n",
    "| `enc2`                   | 16×24            | 64                 | 128 (concat)             |\n",
    "| `pool2`                  | 8×12             | 64                 | 128 (concat)             |\n",
    "| Bottleneck (concat)      | 8×12             | —                  | 256                      |\n",
    "| `dec2` entrada           | 8×12             | 256 + 128 = 384    | —                        |\n",
    "| `dec2` saída             | 16×24            | 64                 | —                        |\n",
    "| `dec1` entrada           | 16×24            | 64 + 64 = 128      | —                        |\n",
    "| `dec1` saída             | 32×48            | 32                 | —                        |\n",
    "| Saída final              | 32×48            | 3                  | —                        |\n",
    "\n",
    "64x96\n",
    "| Bloco                    | Altura × Largura | Canais p/ encoder | Canais Pós-concatenação |\n",
    "|--------------------------|------------------|--------------------|--------------------------|\n",
    "| Entrada                  | 64×96            | 3                  | —                        |\n",
    "| `enc1`                   | 64×96            | 32                 | 64 (concat)              |\n",
    "| `pool1`                  | 32×48            | 32                 | 64 (concat)              |\n",
    "| `enc2`                   | 32×48            | 64                 | 128 (concat)             |\n",
    "| `pool2`                  | 16×24            | 64                 | 128 (concat)             |\n",
    "| Bottleneck (concat)      | 16×24            | —                  | 256                      |\n",
    "| `dec2` entrada           | 16×24            | 256 + 128 = 384    | —                        |\n",
    "| `dec2` saída             | 32×48            | 64                 | —                        |\n",
    "| `dec1` entrada           | 32×48            | 64 + 64 = 128      | —                        |\n",
    "| `dec1` saída             | 64×96            | 32                 | —                        |\n",
    "| Saída final              | 64×96            | 3                  | —                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "119f9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CBAM (Convolutional Block Attention Module)\n",
    "# Aplica atenção canal + espacial separadamente\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.sigmoid_channel = nn.Sigmoid()\n",
    "        self.conv_spatial = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Atenção no canal\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        x_out = x * self.sigmoid_channel(avg_out + max_out)  # salva num novo tensor para não perder o input original\n",
    "\n",
    "        # Atenção espacial\n",
    "        avg_out = torch.mean(x_out, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x_out, dim=1, keepdim=True)\n",
    "        spatial_attention = torch.cat([avg_out, max_out], dim=1)  # [N, 2, H, W]\n",
    "        spatial_attention = self.sigmoid_spatial(self.conv_spatial(spatial_attention))  # [N, 1, H, W]\n",
    "\n",
    "        # Multiplica o resultado da atenção espacial pelo tensor original (com canais corretos)\n",
    "        out = x_out * spatial_attention\n",
    "\n",
    "        return out\n",
    "\n",
    "# Self-Attention simples no bottleneck\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        proj_query = self.query(x).view(B, -1, H * W).permute(0, 2, 1)\n",
    "        proj_key = self.key(x).view(B, -1, H * W)\n",
    "        energy = torch.bmm(proj_query, proj_key)  # matriz de atenção\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "\n",
    "        proj_value = self.value(x).view(B, -1, H * W)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(B, C, H, W)\n",
    "        return self.gamma * out + x\n",
    "\n",
    "# Bloco de codificação padrão\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Bloco de decodificação com upsample + concat + convoluções\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_skip, ch_out):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(ch_in, ch_out, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_out + ch_skip, ch_out, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# Rede UNet com dois encoders, CBAM e self-attention no bottleneck\n",
    "class DualEncoderUNet_CBAM_SA_Small(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_ch=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Dois encoders independentes (parte1 e parte2)\n",
    "        self.enc1_1 = EncoderBlock(in_channels, base_ch)\n",
    "        self.enc2_1 = EncoderBlock(base_ch, base_ch * 2)\n",
    "\n",
    "        self.enc1_2 = EncoderBlock(in_channels, base_ch)\n",
    "        self.enc2_2 = EncoderBlock(base_ch, base_ch * 2)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck com self-attention\n",
    "        self.bottleneck = EncoderBlock(base_ch * 4, base_ch * 4)\n",
    "        self.attn = SelfAttention(base_ch * 4)\n",
    "\n",
    "        # CBAM nas skip connections\n",
    "        self.cbam2 = CBAM(base_ch * 4)\n",
    "        self.cbam1 = CBAM(base_ch * 2)\n",
    "\n",
    "        # Decoder com três parâmetros por bloco\n",
    "        self.dec2 = DecoderBlock(base_ch * 4, base_ch * 4, base_ch * 2)  # 128, 128, 64\n",
    "        self.dec1 = DecoderBlock(base_ch * 2, base_ch * 2, base_ch)      # 64, 64, 32\n",
    "\n",
    "        self.final = nn.Conv2d(base_ch, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Encoder para parte1\n",
    "        e1_1 = self.enc1_1(x1)\n",
    "        e2_1 = self.enc2_1(self.pool(e1_1))\n",
    "\n",
    "        # Encoder para parte2\n",
    "        e1_2 = self.enc1_2(x2)\n",
    "        e2_2 = self.enc2_2(self.pool(e1_2))\n",
    "\n",
    "        # Garantir que as features estejam com mesmas dimensões (por segurança)\n",
    "        if e1_1.shape[2:] != e1_2.shape[2:]:\n",
    "            e1_2 = F.interpolate(e1_2, size=e1_1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        if e2_1.shape[2:] != e2_2.shape[2:]:\n",
    "            e2_2 = F.interpolate(e2_2, size=e2_1.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Bottleneck: concatenação + atenção\n",
    "        b = self.bottleneck(torch.cat([self.pool(e2_1), self.pool(e2_2)], dim=1))\n",
    "        b = self.attn(b)\n",
    "\n",
    "        if debug > 0: print(\"b shape:\", b.shape)\n",
    "        if debug > 0: print(\"skip2 shape:\", self.cbam2(torch.cat([e2_1, e2_2], dim=1)).shape)\n",
    "        if debug > 0: print(\"skip1 shape:\", self.cbam1(torch.cat([e1_1, e1_2], dim=1)).shape)\n",
    "\n",
    "\n",
    "        # Decoder com CBAM nas skip connections\n",
    "        d2 = self.dec2(b, self.cbam2(torch.cat([e2_1, e2_2], dim=1)))\n",
    "        d1 = self.dec1(d2, self.cbam1(torch.cat([e1_1, e1_2], dim=1)))\n",
    "\n",
    "        return torch.sigmoid(self.final(d1))  # saída com valo\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=9):  # Agora espera parte1 (3) + parte2 (3) + target/fake (3)\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(in_channels, 64, normalize=False),  # in_channels = 9\n",
    "            *block(64, 128),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)  # saída do PatchGAN (mapa de decisão)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d415f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_msssim import ssim, ms_ssim\n",
    "import lpips\n",
    "from metrics import compute_all_metrics  # Importa a função de métricas do arquivo metrics.py\n",
    "from tqdm import tqdm\n",
    "\n",
    "debug = 0\n",
    "# from generator import DualEncoderUNet_CBAM_SA_Small  # novo gerador com 2 encoders, CBAM e SelfAttention\n",
    "# from discriminator import PatchDiscriminator  # ou caminho equivalente\n",
    "\n",
    "# LPIPS usa um modelo de rede para comparação perceptual\n",
    "lpips_fn = lpips.LPIPS(net='alex')\n",
    "\n",
    "def carregar_checkpoint_mais_recente(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_epoch\") and f.endswith(\".pt\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"epoch\")[1].split(\".\")[0]))\n",
    "    return os.path.join(checkpoint_dir, checkpoints[-1])\n",
    "\n",
    "\n",
    "def train(\n",
    "    generator, discriminator, dataloader, device, epochs,\n",
    "    save_every, checkpoint_dir, checkpoint_batch_dir,\n",
    "    tensorboard_dir, metrics, lr_g=2e-4, lr_d=2e-4,\n",
    "    lr_min=1e-6, gen_steps_per_batch=1\n",
    "):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_batch_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(tensorboard_dir)\n",
    "\n",
    "    criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "\n",
    "    scheduler_G = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=epochs, eta_min=lr_min)\n",
    "    scheduler_D = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_D, T_max=epochs, eta_min=lr_min)\n",
    "\n",
    "    start_epoch = 0\n",
    "    checkpoint_path = carregar_checkpoint_mais_recente(checkpoint_dir)\n",
    "    if checkpoint_path:\n",
    "        print(f\"🔁 Carregando checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
    "        discriminator.load_state_dict(checkpoint[\"discriminator_state_dict\"])\n",
    "        optimizer_G.load_state_dict(checkpoint[\"optimizer_G_state_dict\"])\n",
    "        optimizer_D.load_state_dict(checkpoint[\"optimizer_D_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"✔️ Retomando a partir da época {start_epoch}\")\n",
    "\n",
    "    last_checkpoint_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for i, ((part1, part2), target) in pbar:            \n",
    "            part1 = part1.to(device)\n",
    "            part2 = part2.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            real_input = torch.cat([part1, part2, target], dim=1)\n",
    "            fake = generator(part1, part2)\n",
    "            fake_input = torch.cat([part1, part2, fake.detach()], dim=1)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            pred_real = discriminator(real_input)\n",
    "            pred_fake = discriminator(fake_input)\n",
    "\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "            loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            for _ in range(gen_steps_per_batch):\n",
    "                fake = generator(part1, part2)\n",
    "                fake_input = torch.cat([part1, part2, fake], dim=1)\n",
    "                optimizer_G.zero_grad()\n",
    "                pred_fake = discriminator(fake_input)\n",
    "                loss_G_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "                loss_G_L1 = criterion_L1(fake, target)\n",
    "                loss_G = 8.0 * loss_G_GAN + 2.0 * loss_G_L1\n",
    "                loss_G.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss_G\": f\"{loss_G.item():.4f}\",\n",
    "                \"loss_D\": f\"{loss_D.item():.4f}\"\n",
    "            })\n",
    "\n",
    "            writer.add_scalar(\"Loss/Generator\", loss_G.item(), epoch * len(dataloader) + i)\n",
    "            writer.add_scalar(\"Loss/Discriminator\", loss_D.item(), epoch * len(dataloader) + i)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                eval_metrics = compute_all_metrics(fake, target, part1, part2, writer, epoch * len(dataloader) + i)\n",
    "                for k, v in eval_metrics.items():\n",
    "                    if v is not None:\n",
    "                        writer.add_scalar(f\"Metrics/{k}\", v, epoch * len(dataloader) + i)\n",
    "\n",
    "            # Checkpoint a cada 10 minutos\n",
    "            if time.time() - last_checkpoint_time > 600:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': i,\n",
    "                    'generator_state_dict': generator.state_dict(),\n",
    "                    'discriminator_state_dict': discriminator.state_dict(),\n",
    "                    'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                    'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                }, os.path.join(checkpoint_batch_dir, f'checkpoint_epoch{epoch}_batch{i}.pt'))\n",
    "                last_checkpoint_time = time.time()\n",
    "\n",
    "        # Fim da época: salvar checkpoint principal\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        }, os.path.join(checkpoint_dir, f'checkpoint_epoch{epoch}.pt'))\n",
    "\n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0828f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/1 [00:00<?, ?it/s]/tmp/ipykernel_353957/2836379704.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  sample = torch.load(self.files[idx])\n",
      "Epoch 1/200: 100%|██████████| 1/1 [00:05<00:00,  5.22s/it, loss_G=2.4001, loss_D=0.7041]\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7aaebeb40720>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/multiprocessing/connection.py\", line 133, in __del__\n",
      "    self._close()\n",
      "  File \"/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "Exception ignored in: <function _ConnectionBase.__del__ at 0x7aaebeb40720>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/multiprocessing/connection.py\", line 133, in __del__\n",
      "    self._close()\n",
      "  File \"/home/prkd/anaconda3/envs/ImageStitching/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m generator \u001b[38;5;241m=\u001b[39m DualEncoderUNet_CBAM_SA_Small()\n\u001b[1;32m     21\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m PatchDiscriminator()\n\u001b[0;32m---> 22\u001b[0m train(generator, discriminator, dataloader, device, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, save_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m,\n\u001b[1;32m     23\u001b[0m       checkpoint_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, checkpoint_batch_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./checkpoints_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m       tensorboard_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./logs/32x48\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, gen_steps_per_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 122\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(generator, discriminator, dataloader, device, epochs, save_every, checkpoint_dir, checkpoint_batch_dir, tensorboard_dir, metrics, lr_g, lr_d, lr_min, gen_steps_per_batch)\u001b[0m\n\u001b[1;32m    119\u001b[0m         last_checkpoint_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# Fim da época: salvar checkpoint principal\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch,\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: generator\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiscriminator_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: discriminator\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_G_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer_G\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_D_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m: optimizer_D\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m    128\u001b[0m }, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m    130\u001b[0m scheduler_G\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    131\u001b[0m scheduler_D\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/torch/serialization.py:652\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 652\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n\u001b[1;32m    653\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/ImageStitching/lib/python3.12/site-packages/torch/serialization.py:886\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[1;32m    885\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[0;32m--> 886\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(name, storage, num_bytes)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "\n",
    "\n",
    "# Montar o caminho do checkpoint e logs para o tensorboard\n",
    "!sshfs prkdvps@64.71.153.122:/home/prkdvps/tensorboard/logs /home/prkd/gan-image-stitching-training/gan_image_stitching_training/logs\n",
    "!sshfs prkdvps@64.71.153.122:/home/prkdvps/tensorboard/checkpoints_epoch /home/prkd/gan-image-stitching-training/gan_image_stitching_training/checkpoints_epoch/\n",
    "!sshfs prkdvps@64.71.153.122:/home/prkdvps/tensorboard/checkpoints_batch/ /home/prkd/gan-image-stitching-training/gan_image_stitching_training/checkpoints_batch/\n",
    "\n",
    "\n",
    "# Hiperparâmetros\n",
    "num_epochs = 100\n",
    "gen_steps_per_batch = 20\n",
    "learning_rate = 2e-4\n",
    "lr_min = 1e-5\n",
    "lr_max = 2e-4\n",
    "log_interval = 600  # em segundos (10 minutos)\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Exemplo de chamada (fora do train.py):\n",
    "generator = DualEncoderUNet_CBAM_SA_Small()\n",
    "discriminator = PatchDiscriminator()\n",
    "train(generator, discriminator, dataloader, device, epochs=200, save_every=600,\n",
    "      checkpoint_dir=\"./checkpoints_epoch\", checkpoint_batch_dir=\"./checkpoints_batch\",\n",
    "      tensorboard_dir=\"./logs/32x48\", metrics=True, gen_steps_per_batch=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualizar_amostra_pt(caminho_pt):\n",
    "    \"\"\"\n",
    "    Visualiza a amostra salva no arquivo .pt no formato esperado:\n",
    "    dicionário com chaves: 'parte1', 'parte2', 'mask', 'groundtruth', 'gradiente'.\n",
    "    Cada tensor é uint8, shape [C, H, W].\n",
    "\n",
    "    Parâmetros:\n",
    "        caminho_pt (str ou Path): caminho do arquivo .pt a ser aberto\n",
    "    \"\"\"\n",
    "    sample = torch.load(caminho_pt)\n",
    "\n",
    "    print(\"Chaves no arquivo:\", list(sample.keys()))\n",
    "    for k, v in sample.items():\n",
    "        print(f\"{k}: shape {v.shape}, dtype {v.dtype}\")\n",
    "\n",
    "    # Converter para formato H x W x C e mostrar com matplotlib\n",
    "    def tensor_to_img(tensor):\n",
    "        # tensor [C, H, W], uint8\n",
    "        img = tensor.permute(1, 2, 0).cpu().numpy()\n",
    "        return img\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    for i, key in enumerate(['parte1', 'parte2', 'mask', 'groundtruth', 'gradiente'], 1):\n",
    "        if key in sample:\n",
    "            img = tensor_to_img(sample[key])\n",
    "            shape_str = sample[key].shape\n",
    "            plt.subplot(2, 3, i)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"{key} - {shape_str}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualizar_amostra_pt(\"./train/000000009286_sample10.pt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageStitching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
