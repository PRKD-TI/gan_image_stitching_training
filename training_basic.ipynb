{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import extrair_zip_train_dir as zipService\n",
    "\n",
    "\n",
    "class ImageStitchingDatasetFiles(Dataset):\n",
    "    def __init__(self, folder_path, use_gradiente=False):\n",
    "        self.folder = Path(folder_path)\n",
    "        self.use_gradiente = use_gradiente\n",
    "        # Lista todos arquivos .pt ordenados\n",
    "        self.files = sorted(self.folder.glob(\"*.pt\"))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = torch.load(self.files[idx])\n",
    "\n",
    "        def to_float_tensor(t):\n",
    "            # uint8 [0..255] -> float32 [0..1]\n",
    "            return t.float() / 255.0\n",
    "\n",
    "        parte1 = to_float_tensor(sample[\"parte1\"])\n",
    "        parte2 = to_float_tensor(sample[\"parte2\"])\n",
    "        groundtruth = to_float_tensor(sample[\"groundtruth\"])\n",
    "\n",
    "        if self.use_gradiente:\n",
    "            gradiente = to_float_tensor(sample[\"gradiente\"])\n",
    "            return (parte1, parte2), groundtruth, gradiente\n",
    "        else:\n",
    "            return (parte1, parte2), groundtruth\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"/root/.ssh/colab_key\"):\n",
    "    os.makedirs(\"/root/.ssh\", exist_ok=True)\n",
    "    print('Execute isso no terminal com a senha', \n",
    "          'scp prkdvps@64.71.153.122:/home/prkdvps/.ssh/colab_key /root/.ssh', \n",
    "          'scp prkdvps@64.71.153.122:/home/prkdvps/.ssh/colab_key /root/.ssh')\n",
    "\n",
    "else:\n",
    "    # Instala o sshfs\n",
    "    !apt-get -qq install sshfs > /dev/null\n",
    "\n",
    "    # Cria diretórios locais\n",
    "    !mkdir -p ./datasetzip ./logs ./checkpoints_epoch ./checkpoints_batch ./utils\n",
    "\n",
    "    !sshfs -o IdentityFile=/root/.ssh/colab_key prkdvps@64.71.153.122:/home/prkdvps/datasetzip ./datasetzip\n",
    "    !sshfs -o IdentityFile=/root/.ssh/colab_key prkdvps@64.71.153.122:/home/prkdvps/tensorboard/logs ./logs\n",
    "    !sshfs -o IdentityFile=/root/.ssh/colab_key prkdvps@64.71.153.122:/home/prkdvps/tensorboard/checkpoints_epoch ./checkpoints_epoch\n",
    "    !sshfs -o IdentityFile=/root/.ssh/colab_key prkdvps@64.71.153.122:/home/prkdvps/tensorboard/checkpoints_batch ./checkpoints_batch\n",
    "    !sshfs -o IdentityFile=/root/.ssh/colab_key prkdvps@64.71.153.122:/home/prkdvps/utils ./utils\n",
    "    !sshfs -o IdentityFile=/root/.ssh/colab_key prkdvps@64.71.153.122:/home/prkdvps/datasetzip ./datasetzip\n",
    "\n",
    "    !cp ./utils/metrics.py /content\n",
    "    !cp ./utils/extrair_zip_train_dir.py /content\n",
    "\n",
    "    !pip install pytorch-msssim\n",
    "    !pip install lpips\n",
    "\n",
    "\n",
    "    filename = \"dataset_48_32.zip\"\n",
    "    !mkdir -p ./datasetzip\n",
    "    !scp prkdvps@64.71.153.122:/home/prkdvps/datasetzip/dataset_48_32.zip ./datasetzip\n",
    "    !rm -r ./train\n",
    "    !mkdir -p ./train\n",
    "\n",
    "    zipService.descompactar_zip_com_progresso(f\"./datasetzip/{filename}\", \"./train\")\n",
    "    dataset = ImageStitchingDatasetFiles(\"./train\", use_gradiente=False)\n",
    "    dataloader = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=4, prefetch_factor=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e39e6bb",
   "metadata": {},
   "source": [
    "32 x 48\n",
    "| Bloco                    | Altura × Largura | Canais p/ encoder | Canais Pós-concatenação |\n",
    "|--------------------------|------------------|--------------------|--------------------------|\n",
    "| Entrada                  | 32×48            | 3                  | —                        |\n",
    "| `enc1`                   | 32×48            | 32                 | 64 (concat)              |\n",
    "| `pool1`                  | 16×24            | 32                 | 64 (concat)              |\n",
    "| `enc2`                   | 16×24            | 64                 | 128 (concat)             |\n",
    "| `pool2`                  | 8×12             | 64                 | 128 (concat)             |\n",
    "| Bottleneck (concat)      | 8×12             | —                  | 256                      |\n",
    "| `dec2` entrada           | 8×12             | 256 + 128 = 384    | —                        |\n",
    "| `dec2` saída             | 16×24            | 64                 | —                        |\n",
    "| `dec1` entrada           | 16×24            | 64 + 64 = 128      | —                        |\n",
    "| `dec1` saída             | 32×48            | 32                 | —                        |\n",
    "| Saída final              | 32×48            | 3                  | —                        |\n",
    "\n",
    "64x96\n",
    "| Bloco                    | Altura × Largura | Canais p/ encoder | Canais Pós-concatenação |\n",
    "|--------------------------|------------------|--------------------|--------------------------|\n",
    "| Entrada                  | 64×96            | 3                  | —                        |\n",
    "| `enc1`                   | 64×96            | 32                 | 64 (concat)              |\n",
    "| `pool1`                  | 32×48            | 32                 | 64 (concat)              |\n",
    "| `enc2`                   | 32×48            | 64                 | 128 (concat)             |\n",
    "| `pool2`                  | 16×24            | 64                 | 128 (concat)             |\n",
    "| Bottleneck (concat)      | 16×24            | —                  | 256                      |\n",
    "| `dec2` entrada           | 16×24            | 256 + 128 = 384    | —                        |\n",
    "| `dec2` saída             | 32×48            | 64                 | —                        |\n",
    "| `dec1` entrada           | 32×48            | 64 + 64 = 128      | —                        |\n",
    "| `dec1` saída             | 64×96            | 32                 | —                        |\n",
    "| Saída final              | 64×96            | 3                  | —                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f9349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# CBAM (Convolutional Block Attention Module)\n",
    "# Aplica atenção canal + espacial separadamente\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(CBAM, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, 1, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels // reduction, channels, 1, bias=False)\n",
    "        )\n",
    "\n",
    "        self.sigmoid_channel = nn.Sigmoid()\n",
    "        self.conv_spatial = nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False)\n",
    "        self.sigmoid_spatial = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Atenção no canal\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        x_out = x * self.sigmoid_channel(avg_out + max_out)  # salva num novo tensor para não perder o input original\n",
    "\n",
    "        # Atenção espacial\n",
    "        avg_out = torch.mean(x_out, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x_out, dim=1, keepdim=True)\n",
    "        spatial_attention = torch.cat([avg_out, max_out], dim=1)  # [N, 2, H, W]\n",
    "        spatial_attention = self.sigmoid_spatial(self.conv_spatial(spatial_attention))  # [N, 1, H, W]\n",
    "\n",
    "        # Multiplica o resultado da atenção espacial pelo tensor original (com canais corretos)\n",
    "        out = x_out * spatial_attention\n",
    "\n",
    "        return out\n",
    "\n",
    "# Self-Attention simples no bottleneck\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.key = nn.Conv2d(in_dim, in_dim // 8, 1)\n",
    "        self.value = nn.Conv2d(in_dim, in_dim, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        proj_query = self.query(x).view(B, -1, H * W).permute(0, 2, 1)\n",
    "        proj_key = self.key(x).view(B, -1, H * W)\n",
    "        energy = torch.bmm(proj_query, proj_key)  # matriz de atenção\n",
    "        attention = F.softmax(energy, dim=-1)\n",
    "\n",
    "        proj_value = self.value(x).view(B, -1, H * W)\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(B, C, H, W)\n",
    "        return self.gamma * out + x\n",
    "\n",
    "# Bloco de codificação padrão\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Bloco de decodificação com upsample + concat + convoluções\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, ch_in, ch_skip, ch_out):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(ch_in, ch_out, kernel_size=2, stride=2)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_out + ch_skip, ch_out, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, 3, padding=1),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = self.up(x)\n",
    "        if x.shape[2:] != skip.shape[2:]:\n",
    "            x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=False)\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# Rede UNet com dois encoders, CBAM e self-attention no bottleneck\n",
    "class DualEncoderUNet_CBAM_SA_Small(nn.Module):\n",
    "    def __init__(self, in_channels=3, base_ch=32):\n",
    "        super().__init__()\n",
    "\n",
    "        # Dois encoders independentes (parte1 e parte2)\n",
    "        self.enc1_1 = EncoderBlock(in_channels, base_ch)\n",
    "        self.enc2_1 = EncoderBlock(base_ch, base_ch * 2)\n",
    "\n",
    "        self.enc1_2 = EncoderBlock(in_channels, base_ch)\n",
    "        self.enc2_2 = EncoderBlock(base_ch, base_ch * 2)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Bottleneck com self-attention\n",
    "        self.bottleneck = EncoderBlock(base_ch * 4, base_ch * 4)\n",
    "        self.attn = SelfAttention(base_ch * 4)\n",
    "\n",
    "        # CBAM nas skip connections\n",
    "        self.cbam2 = CBAM(base_ch * 4)\n",
    "        self.cbam1 = CBAM(base_ch * 2)\n",
    "\n",
    "        # Decoder com três parâmetros por bloco\n",
    "        self.dec2 = DecoderBlock(base_ch * 4, base_ch * 4, base_ch * 2)  # 128, 128, 64\n",
    "        self.dec1 = DecoderBlock(base_ch * 2, base_ch * 2, base_ch)      # 64, 64, 32\n",
    "\n",
    "        self.final = nn.Conv2d(base_ch, 3, kernel_size=1)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # Encoder para parte1\n",
    "        e1_1 = self.enc1_1(x1)\n",
    "        e2_1 = self.enc2_1(self.pool(e1_1))\n",
    "\n",
    "        # Encoder para parte2\n",
    "        e1_2 = self.enc1_2(x2)\n",
    "        e2_2 = self.enc2_2(self.pool(e1_2))\n",
    "\n",
    "        # Garantir que as features estejam com mesmas dimensões (por segurança)\n",
    "        if e1_1.shape[2:] != e1_2.shape[2:]:\n",
    "            e1_2 = F.interpolate(e1_2, size=e1_1.shape[2:], mode='bilinear', align_corners=False)\n",
    "        if e2_1.shape[2:] != e2_2.shape[2:]:\n",
    "            e2_2 = F.interpolate(e2_2, size=e2_1.shape[2:], mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Bottleneck: concatenação + atenção\n",
    "        b = self.bottleneck(torch.cat([self.pool(e2_1), self.pool(e2_2)], dim=1))\n",
    "        b = self.attn(b)\n",
    "\n",
    "        if debug > 0: print(\"b shape:\", b.shape)\n",
    "        if debug > 0: print(\"skip2 shape:\", self.cbam2(torch.cat([e2_1, e2_2], dim=1)).shape)\n",
    "        if debug > 0: print(\"skip1 shape:\", self.cbam1(torch.cat([e1_1, e1_2], dim=1)).shape)\n",
    "\n",
    "\n",
    "        # Decoder com CBAM nas skip connections\n",
    "        d2 = self.dec2(b, self.cbam2(torch.cat([e2_1, e2_2], dim=1)))\n",
    "        d1 = self.dec1(d2, self.cbam1(torch.cat([e1_1, e1_2], dim=1)))\n",
    "\n",
    "        return torch.sigmoid(self.final(d1))  # saída com valo\n",
    "import torch.nn as nn\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, in_channels=9):  # Agora espera parte1 (3) + parte2 (3) + target/fake (3)\n",
    "        super(PatchDiscriminator, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Conv2d(in_feat, out_feat, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm2d(out_feat))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *block(in_channels, 64, normalize=False),  # in_channels = 9\n",
    "            *block(64, 128),\n",
    "            *block(128, 256),\n",
    "            *block(256, 512),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1)  # saída do PatchGAN (mapa de decisão)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d415f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pytorch_msssim import ssim, ms_ssim\n",
    "import lpips\n",
    "from metrics import compute_all_metrics  # Importa a função de métricas do arquivo metrics.py\n",
    "from tqdm import tqdm\n",
    "\n",
    "debug = 0\n",
    "# from generator import DualEncoderUNet_CBAM_SA_Small  # novo gerador com 2 encoders, CBAM e SelfAttention\n",
    "# from discriminator import PatchDiscriminator  # ou caminho equivalente\n",
    "\n",
    "# LPIPS usa um modelo de rede para comparação perceptual\n",
    "lpips_fn = lpips.LPIPS(net='alex')\n",
    "\n",
    "def carregar_checkpoint_mais_recente(checkpoint_dir):\n",
    "    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_epoch\") and f.endswith(\".pt\")]\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "\n",
    "    checkpoints.sort(key=lambda x: int(x.split(\"epoch\")[1].split(\".\")[0]))\n",
    "    return os.path.join(checkpoint_dir, checkpoints[-1])\n",
    "\n",
    "\n",
    "def train(\n",
    "    generator, discriminator, dataloader, device, epochs,\n",
    "    save_every, checkpoint_dir, checkpoint_batch_dir,\n",
    "    tensorboard_dir, metrics, lr_g=2e-4, lr_d=2e-4,\n",
    "    lr_min=1e-6, gen_steps_per_batch=1\n",
    "):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_batch_dir, exist_ok=True)\n",
    "    writer = SummaryWriter(tensorboard_dir)\n",
    "\n",
    "    criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "    criterion_L1 = nn.L1Loss()\n",
    "\n",
    "    optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr_g, betas=(0.5, 0.999))\n",
    "    optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr_d, betas=(0.5, 0.999))\n",
    "\n",
    "    scheduler_G = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=epochs, eta_min=lr_min)\n",
    "    scheduler_D = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_D, T_max=epochs, eta_min=lr_min)\n",
    "\n",
    "    start_epoch = 0\n",
    "    checkpoint_path = carregar_checkpoint_mais_recente(checkpoint_dir)\n",
    "    if checkpoint_path:\n",
    "        print(f\"🔁 Carregando checkpoint: {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        generator.load_state_dict(checkpoint[\"generator_state_dict\"])\n",
    "        discriminator.load_state_dict(checkpoint[\"discriminator_state_dict\"])\n",
    "        optimizer_G.load_state_dict(checkpoint[\"optimizer_G_state_dict\"])\n",
    "        optimizer_D.load_state_dict(checkpoint[\"optimizer_D_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        print(f\"✔️ Retomando a partir da época {start_epoch}\")\n",
    "\n",
    "    last_checkpoint_time = time.time()\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for i, ((part1, part2), target) in pbar:            \n",
    "            part1 = part1.to(device)\n",
    "            part2 = part2.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            real_input = torch.cat([part1, part2, target], dim=1)\n",
    "            fake = generator(part1, part2)\n",
    "            fake_input = torch.cat([part1, part2, fake.detach()], dim=1)\n",
    "\n",
    "            # Train Discriminator\n",
    "            optimizer_D.zero_grad()\n",
    "            pred_real = discriminator(real_input)\n",
    "            pred_fake = discriminator(fake_input)\n",
    "\n",
    "            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))\n",
    "            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))\n",
    "            loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # Train Generator\n",
    "            for _ in range(gen_steps_per_batch):\n",
    "                fake = generator(part1, part2)\n",
    "                fake_input = torch.cat([part1, part2, fake], dim=1)\n",
    "                optimizer_G.zero_grad()\n",
    "                pred_fake = discriminator(fake_input)\n",
    "                loss_G_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "                loss_G_L1 = criterion_L1(fake, target)\n",
    "                loss_G = 8.0 * loss_G_GAN + 2.0 * loss_G_L1\n",
    "                loss_G.backward()\n",
    "                optimizer_G.step()\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"loss_G\": f\"{loss_G.item():.4f}\",\n",
    "                \"loss_D\": f\"{loss_D.item():.4f}\"\n",
    "            })\n",
    "\n",
    "            writer.add_scalar(\"Loss/Generator\", loss_G.item(), epoch * len(dataloader) + i)\n",
    "            writer.add_scalar(\"Loss/Discriminator\", loss_D.item(), epoch * len(dataloader) + i)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                eval_metrics = compute_all_metrics(fake, target, part1, part2, writer, epoch * len(dataloader) + i)\n",
    "                for k, v in eval_metrics.items():\n",
    "                    if v is not None:\n",
    "                        writer.add_scalar(f\"Metrics/{k}\", v, epoch * len(dataloader) + i)\n",
    "\n",
    "            # Checkpoint a cada 10 minutos\n",
    "            if time.time() - last_checkpoint_time > 600:\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'batch': i,\n",
    "                    'generator_state_dict': generator.state_dict(),\n",
    "                    'discriminator_state_dict': discriminator.state_dict(),\n",
    "                    'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "                    'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "                }, os.path.join(checkpoint_batch_dir, f'checkpoint_epoch{epoch}_batch{i}.pt'))\n",
    "                last_checkpoint_time = time.time()\n",
    "\n",
    "        # Fim da época: salvar checkpoint principal\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator_state_dict': generator.state_dict(),\n",
    "            'discriminator_state_dict': discriminator.state_dict(),\n",
    "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
    "            'optimizer_D_state_dict': optimizer_D.state_dict(),\n",
    "        }, os.path.join(checkpoint_dir, f'checkpoint_epoch{epoch}.pt'))\n",
    "\n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0828f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"torch\")\n",
    "\n",
    "\n",
    "# Montar o caminho do checkpoint e logs para o tensorboard\n",
    "!sshfs prkdvps@64.71.153.122:/home/prkdvps/tensorboard/logs /home/prkd/gan-image-stitching-training/gan_image_stitching_training/logs\n",
    "!sshfs prkdvps@64.71.153.122:/home/prkdvps/tensorboard/checkpoints_epoch /home/prkd/gan-image-stitching-training/gan_image_stitching_training/checkpoints_epoch/\n",
    "!sshfs prkdvps@64.71.153.122:/home/prkdvps/tensorboard/checkpoints_batch/ /home/prkd/gan-image-stitching-training/gan_image_stitching_training/checkpoints_batch/\n",
    "\n",
    "debug = 0\n",
    "\n",
    "# Hiperparâmetros\n",
    "num_epochs = 100\n",
    "gen_steps_per_batch = 20\n",
    "learning_rate = 2e-4\n",
    "lr_min = 1e-5\n",
    "lr_max = 2e-4\n",
    "log_interval = 600  # em segundos (10 minutos)\n",
    "\n",
    "# Dispositivo\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Exemplo de chamada (fora do train.py):\n",
    "generator = DualEncoderUNet_CBAM_SA_Small().to(device)\n",
    "discriminator = PatchDiscriminator().to(device)\n",
    "train(generator, discriminator, dataloader, device, epochs=200, save_every=600,\n",
    "      checkpoint_dir=\"./checkpoints_epoch\", checkpoint_batch_dir=\"./checkpoints_batch\",\n",
    "      tensorboard_dir=\"./logs/32x48\", metrics=True, gen_steps_per_batch=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca74f00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualizar_amostra_pt(caminho_pt):\n",
    "    \"\"\"\n",
    "    Visualiza a amostra salva no arquivo .pt no formato esperado:\n",
    "    dicionário com chaves: 'parte1', 'parte2', 'mask', 'groundtruth', 'gradiente'.\n",
    "    Cada tensor é uint8, shape [C, H, W].\n",
    "\n",
    "    Parâmetros:\n",
    "        caminho_pt (str ou Path): caminho do arquivo .pt a ser aberto\n",
    "    \"\"\"\n",
    "    sample = torch.load(caminho_pt)\n",
    "\n",
    "    print(\"Chaves no arquivo:\", list(sample.keys()))\n",
    "    for k, v in sample.items():\n",
    "        print(f\"{k}: shape {v.shape}, dtype {v.dtype}\")\n",
    "\n",
    "    # Converter para formato H x W x C e mostrar com matplotlib\n",
    "    def tensor_to_img(tensor):\n",
    "        # tensor [C, H, W], uint8\n",
    "        img = tensor.permute(1, 2, 0).cpu().numpy()\n",
    "        return img\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    for i, key in enumerate(['parte1', 'parte2', 'mask', 'groundtruth', 'gradiente'], 1):\n",
    "        if key in sample:\n",
    "            img = tensor_to_img(sample[key])\n",
    "            shape_str = sample[key].shape\n",
    "            plt.subplot(2, 3, i)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"{key} - {shape_str}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualizar_amostra_pt(\"./train/000000009286_sample10.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9234ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔐 Variáveis com o conteúdo da chave pública e privada\n",
    "PRIVATE_KEY = \"\"\"\n",
    "-----BEGIN OPENSSH PRIVATE KEY-----\n",
    "b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAAAMwAAAAtzc2gtZW\n",
    "QyNTUxOQAAACDhMdKyaW9Q6h7gbSNiEccuYXUrUS9PekHNCwnSdKkliwAAAJBX9BMSV/QT\n",
    "EgAAAAtzc2gtZWQyNTUxOQAAACDhMdKyaW9Q6h7gbSNiEccuYXUrUS9PekHNCwnSdKkliw\n",
    "AAAEACeDr6P/5M1e73MfCFezLbib6MTEvwrqYGLqxMMQB/dOEx0rJpb1DqHuBtI2IRxy5h\n",
    "dStRL096Qc0LCdJ0qSWLAAAADGNvbGFiLWFjY2VzcwE=\n",
    "-----END OPENSSH PRIVATE KEY-----\n",
    "\"\"\"\n",
    "\n",
    "PUBLIC_KEY = \"ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOEx0rJpb1DqHuBtI2IRxy5hdStRL096Qc0LCdJ0qSWL colab-access\"\n",
    "\n",
    "REMOTE_USER = \"prkdvps\"\n",
    "REMOTE_HOST = \"64.71.153.122\"\n",
    "KEY_PATH = \"/root/.ssh/colab_key\"\n",
    "\n",
    "import os\n",
    "\n",
    "# Criação da pasta .ssh\n",
    "os.makedirs(\"/root/.ssh\", exist_ok=True)\n",
    "\n",
    "# Escreve chave privada\n",
    "with open(KEY_PATH, \"w\") as f:\n",
    "    f.write(PRIVATE_KEY.strip())\n",
    "\n",
    "# Escreve chave pública\n",
    "with open(\"/root/.ssh/colab_key.pub\", \"w\") as f:\n",
    "    f.write(PUBLIC_KEY.strip())\n",
    "\n",
    "# Ajusta permissões\n",
    "!chmod 600 /root/.ssh/colab_key\n",
    "!chmod 644 /root/.ssh/colab_key.pub\n",
    "\n",
    "# Adiciona o host remoto no known_hosts\n",
    "!ssh-keyscan -H $REMOTE_HOST >> /root/.ssh/known_hosts\n",
    "\n",
    "# Instala o sshfs\n",
    "!apt-get -qq install sshfs > /dev/null\n",
    "\n",
    "# Cria diretórios locais\n",
    "!mkdir -p ./datasetzip ./logs ./checkpoints_epoch ./checkpoints_batch ./utils\n",
    "\n",
    "!sshfs -o IdentityFile=~/.ssh/colab_key prkdvps@64.71.153.122:/home/prkdvps/datasetzip ./datasetzip\n",
    "!sshfs -o IdentityFile=~/.ssh/colab_key prkdvps@64.71.153.122:/home/prkdvps/tensorboard/logs ./logs\n",
    "!sshfs -o IdentityFile=~/.ssh/colab_key prkdvps@64.71.153.122:/home/prkdvps/tensorboard/checkpoints_epoch ./checkpoints_epoch\n",
    "!sshfs -o IdentityFile=~/.ssh/colab_key prkdvps@64.71.153.122:/home/prkdvps/tensorboard/checkpoints_batch ./checkpoints_batch\n",
    "!sshfs -o IdentityFile=~/.ssh/colab_key prkdvps@64.71.153.122:/home/prkdvps/utils ./utils\n",
    "\n",
    "!cp ./utils/metrics.py /content\n",
    "!cp ./utils/extrair_zip_train_dir.py /content\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageStitching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
